{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bone Metastasis Classification using Base CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Irish\\anaconda3\\envs\\conv_env\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchstat import stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Variables & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2b66ac59c70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed for random number generation to create reproducible results\n",
    "random_seed = 5\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations to apply to inputs\n",
    "preprocess = transforms.Compose([\n",
    "    # Convert PIL Image to tensor and scale to [0, 1] through max normalization\n",
    "    # (i.e. for every pixel in image, new_pixel_value = pixel/255)\n",
    "    transforms.ToTensor(),\n",
    "    # Resize to 256 x 256\n",
    "    transforms.Resize((256, 256))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get ground truth class of an image\n",
    "def get_img_labels(img_dir):\n",
    "    labels = ''\n",
    "    \n",
    "    for filename in os.listdir(img_dir):\n",
    "        # If image has no metastasis\n",
    "        if (filename[5] == '0'):\n",
    "            labels += filename + \",0\\n\"\n",
    "            \n",
    "        # If image has metastasis\n",
    "        else:\n",
    "            labels += filename + \",1\\n\"\n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r\"dataset-augmented\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_labels = pd.read_csv(StringIO(get_img_labels(img_dir)), sep=\",\", header=None)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate image filepath\n",
    "        filename = self.img_labels.iloc[idx, 0]\n",
    "        img_path = self.img_dir + \"/\" + filename\n",
    "        \n",
    "        # Read and transform image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        # Determine ground truth class (metastasis or no metastasis)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and dataloader for training data\n",
    "dataset = CustomDataset(img_dir=data_dir, transform=preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 3\n",
    "\n",
    "# Create model with CNN and MLP for classification\n",
    "class CNN(nn.Module):\n",
    "    # Define layers\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels = channels, out_channels = channels, kernel_size = 3)\n",
    "        self.max_pooling2d_1 = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.dropout_1 = nn.Dropout(0.2)\n",
    "        self.conv2d_2 = nn.Conv2d(in_channels = channels, out_channels = channels, kernel_size = 3)\n",
    "        self.max_pooling2d_2 = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.dropout_2 = nn.Dropout(0.2)\n",
    "        self.conv2d_3 = nn.Conv2d(in_channels = channels, out_channels = channels, kernel_size = 3)\n",
    "        self.max_pooling2d_3 = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.dropout_3 = nn.Dropout(0.2)\n",
    "        self.dropout_4 = nn.Dropout(0.2)\n",
    "        self.dense_1 = nn.Linear(in_features = channels * 158 * 52, out_features = 64)\n",
    "        self.dense_2 = nn.Linear(in_features = 64, out_features = 100)\n",
    "\n",
    "    # Apply layers\n",
    "    def forward(self, x):\n",
    "        # Convolution layers\n",
    "        x = F.relu(self.conv2d_1(x))\n",
    "        x = self.max_pooling2d_1(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = F.relu(self.conv2d_2(x))\n",
    "        x = self.max_pooling2d_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = F.relu(self.conv2d_3(x))\n",
    "        x = self.max_pooling2d_3(x)\n",
    "        x = self.dropout_3(x)\n",
    "        \n",
    "        # Flatten all dimensions except batch\n",
    "        x = torch.flatten(input = x, start_dim = 1)\n",
    "        \n",
    "        # Classification layers\n",
    "        x = self.dropout_4(x)\n",
    "        x = F.relu(self.dense_1(x))\n",
    "        x = F.sigmoid(self.dense_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing torch operations on cpu device\n"
     ]
    }
   ],
   "source": [
    "# Allocate tensors to the device used for computation\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Performing torch operations on {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define criterion (function used to compute loss) and optimizer for model\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters; epochs & batch size from Magboo & Abu\n",
    "k_folds = 2 # TODO: Change to 10\n",
    "epochs = 2 # TODO: Change to 100\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics\n",
    "results_accuracy = []\n",
    "results_precision = []\n",
    "results_sensitivity = []\n",
    "results_specificity = []\n",
    "results_f1 = []\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "train_set, test_set = train_test_split(dataset, train_size=0.8, random_state=random_seed, shuffle=True, stratify=dataset)\n",
    "    \n",
    "# Get DataLoaders for training and test sets\n",
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x2700 and 24648x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m batch_data\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Pass input through model.forward()\u001b[39;00m\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, labels) \u001b[38;5;66;03m# Compute loss against ground truth labels\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\conv_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[8], line 39\u001b[0m, in \u001b[0;36mCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Classification layers\u001b[39;00m\n\u001b[0;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_4(x)\n\u001b[1;32m---> 39\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_2(x))\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\conv_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\conv_env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x2700 and 24648x64)"
     ]
    }
   ],
   "source": [
    "# Train for e epochs\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0\n",
    "    total_batch_count = 0\n",
    "        \n",
    "    for batch_index, batch_data in enumerate(trainloader):\n",
    "        # Get the inputs; data is a list of [images, labels]\n",
    "        images, labels = batch_data\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(images) # Pass input through model.forward()\n",
    "        loss = criterion(predictions, labels) # Compute loss against ground truth labels\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad() # Reset gradients from previous passes\n",
    "        loss.backward() # Compute gradients using derivative of loss\n",
    "        optimizer.step() # Update values using gradients\n",
    "\n",
    "        running_loss += loss.item() # Add batch loss to current epoch loss\n",
    "        total_batch_count += 1 # Increment number of finished batches\n",
    "\n",
    "    running_loss = running_loss / total_batch_count\n",
    "    losses.append(running_loss)\n",
    "    print(f\"Epoch {epoch}/{epochs-1} Loss: {running_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print tensor sizes per layer in model\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'weights/lbfcnn_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "ax = plt.axes()\n",
    "plt.plot(losses, color='red')\n",
    "plt.title('Training Loss Evaluation')\n",
    "ax.set_xticks([x for x in range(epochs)])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Magnitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation for this fold\n",
    "true_pos = 0\n",
    "true_neg = 0\n",
    "false_pos = 0\n",
    "false_neg = 0\n",
    "total = 0\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for batch_index, batch_data in enumerate(testloader):\n",
    "        # Get the inputs; data is a list of [images, labels]\n",
    "        images, labels = batch_data\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(images)\n",
    "\n",
    "        # Set total and correct\n",
    "        _, predicted = torch.max(input=predictions, dim=1) # Get list of predicted classes\n",
    "            \n",
    "        # Get how many classes there were in this batch\n",
    "        total += labels.size(0)\n",
    "            \n",
    "        # Get true positive, true negative, false positive, and false negative counts\n",
    "        for index in range(len(labels)):\n",
    "            true_pos = true_pos+1 if (predicted[index] == 1 and labels[index] == 1) else true_pos\n",
    "            true_neg = true_neg+1 if (predicted[index] == 0 and labels[index] == 0) else true_neg\n",
    "            false_pos = false_pos+1 if (predicted[index] == 1 and labels[index] == 0) else false_pos\n",
    "            false_neg = false_neg+1 if (predicted[index] == 0 and labels[index] == 1) else false_neg\n",
    "    \n",
    "print(f\"TP: {true_pos}, TN: {true_neg}, FP: {false_pos}, FN: {false_neg}, total: {total}\")\n",
    "print(\"Final Performance Metrics\")\n",
    "    \n",
    "# Get evaluation metrics\n",
    "# accuracy: (tp + tn) / (p + n)\n",
    "accuracy = (true_pos + true_neg)/total if total != 0 else 0\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "    \n",
    "# precision tp / (tp + fp)\n",
    "precision = true_pos/(true_pos + false_pos) if (true_pos + false_pos) != 0 else 0\n",
    "print(f\"Precision: {precision}\")\n",
    "    \n",
    "# sensitivity: tp / (tp + fn)\n",
    "sensitivity = true_pos/(true_pos + false_neg) if (true_pos + false_neg) != 0 else 0\n",
    "print(f\"Sensitivity: {sensitivity}\")\n",
    "    \n",
    "# specificity: tn / (tn + fp)\n",
    "specificity = true_neg/(true_neg + false_pos) if (true_neg + false_pos) != 0 else 0\n",
    "print(f\"Specificity: {specificity}\")\n",
    "    \n",
    "# f1: 2(precision * recall)/(precision + recall)\n",
    "f1 = 2 * (precision * sensitivity)/(precision + sensitivity) if (precision + sensitivity) != 0 else 0\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get computational complexity\n",
    "stat(model, (1, 646, 220))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Predictions on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img_names = [\"0000-0-A.tif\", \"0163-0-P.tif\", \"0198-0-A.tif\"]\n",
    "sample_img_paths = [(data_dir + \"/\" + sample_img_names[i]) for i in range(len(sample_img_names))]\n",
    "sample_classes = [sample_img_names[i][5] for i in range(len(sample_img_names))]\n",
    "sample_ground_truths = [\"No Metastasis\" if sample_classes[i] == 0 else \"Metastasis\" for i in range(len(sample_img_names))]\n",
    "sample_imgs = [Image.open(img_path).convert('RGB') for img_path in sample_img_paths]\n",
    "sample_imgs_show = [Image.open(img_path) for img_path in sample_img_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_batch = torch.stack([preprocess(img).to(device) for img in sample_imgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_preds= model(validation_batch).data.numpy()\n",
    "sample_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(sample_imgs_show), figsize=(20, 5))\n",
    "for i, img in enumerate(sample_imgs_show):\n",
    "    ax = axs[i]\n",
    "    ax.axis('off')\n",
    "    ax.set_title(\"Prediction: {:.0f}% No Metastasis, {:.0f}% Metastasis\\n Ground Truth: {}\"\n",
    "                 .format(100*sample_preds[i,0], 100*sample_preds[i,1], sample_ground_truths[i]))\n",
    "    ax.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
