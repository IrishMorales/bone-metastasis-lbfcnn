{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a818a28",
   "metadata": {},
   "source": [
    "# Data Shuffling and Stratified Train-Val-Test Split Sample Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb5dcb2",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf2c3f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Irish\\anaconda3\\envs\\conv_env\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] The specified procedure could not be found\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc622cff",
   "metadata": {},
   "source": [
    "### Define Variables & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a3f3329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28ab84ddc90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seed for random number generation to create reproducible results\n",
    "random_seed = 5\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "409a7e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations to apply to inputs\n",
    "preprocess = transforms.Compose([\n",
    "    # Convert PIL Image to tensor and scale to [0, 1] through max normalization\n",
    "    # (i.e. for every pixel in image, new_pixel_value = pixel/255)\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676a0614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get ground truth class of an image\n",
    "def get_img_labels(img_dir):\n",
    "    labels = ''\n",
    "    \n",
    "    for filename in os.listdir(img_dir):\n",
    "        # If image has no metastasis\n",
    "        if (filename[5] == '0'):\n",
    "            labels += filename + \",0\\n\"\n",
    "            \n",
    "        # If image has metastasis\n",
    "        else:\n",
    "            labels += filename + \",1\\n\"\n",
    "        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe98a8e",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba0ca4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_labels = pd.read_csv(StringIO(get_img_labels(img_dir)), sep=\",\", header=None)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate image filepath\n",
    "        filename = self.img_labels.iloc[idx, 0]\n",
    "        img_path = self.img_dir + \"/\" + filename\n",
    "        \n",
    "        # Read and transform image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        # Determine ground truth class (metastasis or no metastasis)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        return image, label, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68e05b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and dataloader for training data\n",
    "dataset = CustomDataset(img_dir=r\"dataset-augmented\", transform=preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9844621",
   "metadata": {},
   "source": [
    "### Data Shuffling & Stratified Train-Val-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9eb7bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classes and indices of images\n",
    "labels = list(dataset.img_labels.iloc[:, 1])\n",
    "indices = np.arange(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad71a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into 70% training, 10% validation, and 20% test with shuffle and stratified split\n",
    "def get_set_indices(random_seed):\n",
    "    # Split dataset into training and test sets\n",
    "    train_indices, test_indices = train_test_split(indices, \n",
    "                                           train_size=0.8, \n",
    "                                           random_state=random_seed,\n",
    "                                           shuffle=True, \n",
    "                                           stratify=labels)\n",
    "\n",
    "    # Split training set into training and validation sets\n",
    "    train_labels = [labels[i] for i in train_indices]\n",
    "    train_indices, val_indices = train_test_split(train_indices, \n",
    "                                          train_size=0.875,\n",
    "                                          random_state=random_seed, \n",
    "                                          shuffle=True, \n",
    "                                          stratify=train_labels)\n",
    "\n",
    "    # Check that dataset was split into 70% training, 10% validation, and 20% test\n",
    "    print(f\"Images in training set: {len(train_indices)} ({(len(train_indices)*100/len(indices)):.2f}% of total dataset)\")\n",
    "    print(f\"Images in validation set: {len(val_indices)} ({(len(val_indices)*100/len(indices)):.2f}% of total dataset)\")\n",
    "    print(f\"Images in test set: {len(test_indices)} ({(len(test_indices)*100/len(indices)):.2f}% of total dataset)\")\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"Total images in dataset: {len(indices)}\")\n",
    "    \n",
    "    return train_indices, val_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57f4389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that training, validation, and test sets have the same ratio of non-metastasis to metastasis as the whole dataset\n",
    "def check_set_ratios(train_indices, test_indices, val_indices):\n",
    "    train_labels = [labels[i] for i in train_indices]\n",
    "    val_labels = [labels[i] for i in val_indices]\n",
    "    test_labels = [labels[i] for i in test_indices]\n",
    "\n",
    "    train_classes, train_counts = np.unique(train_labels, return_counts=True)\n",
    "    val_classes, val_counts = np.unique(val_labels, return_counts=True)\n",
    "    test_classes, test_counts = np.unique(test_labels, return_counts=True)\n",
    "    dataset_classes, dataset_counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    print(f\"Classes in training set:\")\n",
    "    print(f\"{train_counts[0]} without metastasis ({(train_counts[0]*100/len(train_indices)):.2f}% of training set)\")\n",
    "    print(f\"{train_counts[1]} with metastasis ({(train_counts[1]*100/len(train_indices)):.2f}% of training set)\\n\")\n",
    "    print(f\"Classes in validation set:\")\n",
    "    print(f\"{val_counts[0]} without metastasis ({(val_counts[0]*100/len(val_indices)):.2f}% of validation set)\")\n",
    "    print(f\"{val_counts[1]} with metastasis ({(val_counts[1]*100/len(val_indices)):.2f}% of validation set)\\n\")\n",
    "    print(f\"Classes in test set:\")\n",
    "    print(f\"{test_counts[0]} without metastasis ({(test_counts[0]*100/len(test_indices)):.2f}% of test set)\")\n",
    "    print(f\"{test_counts[1]} with metastasis ({(test_counts[1]*100/len(test_indices)):.2f}% of test set)\")\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"Total classes in dataset:\")\n",
    "    print(f\"{dataset_counts[0]} without metastasis ({(dataset_counts[0]*100/len(indices)):.2f}% of dataset)\")\n",
    "    print(f\"{dataset_counts[1]} with metastasis ({(dataset_counts[1]*100/len(indices)):.2f}% of dataset)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ee5e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training, validation, and test sets\n",
    "def get_sets(train_indices, val_indices, test_indices):\n",
    "    train_set = Subset(dataset, train_indices)\n",
    "    val_set = Subset(dataset, val_indices)\n",
    "    test_set = Subset(dataset, test_indices)\n",
    "    \n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6dbb957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training, validation, and test sets\n",
    "def get_loaders(train_set, val_set, test_set, batch_size):\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec4a9b4",
   "metadata": {},
   "source": [
    "### Verify Data Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2290c74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/2\n",
      "Images in training set: 1715 (69.97% of total dataset)\n",
      "Images in validation set: 245 (10.00% of total dataset)\n",
      "Images in test set: 491 (20.03% of total dataset)\n",
      "----------\n",
      "Total images in dataset: 2451\n",
      "Classes in training set:\n",
      "1117 without metastasis (65.13% of training set)\n",
      "598 with metastasis (34.87% of training set)\n",
      "\n",
      "Classes in validation set:\n",
      "320 without metastasis (65.17% of validation set)\n",
      "171 with metastasis (34.83% of validation set)\n",
      "\n",
      "Classes in test set:\n",
      "160 without metastasis (65.31% of test set)\n",
      "85 with metastasis (34.69% of test set)\n",
      "----------\n",
      "Total classes in dataset:\n",
      "1597 without metastasis (65.16% of dataset)\n",
      "854 with metastasis (34.84% of dataset)\n",
      "Training Batch 0 Classes: 1 0 0 1 0 0 0 1 0 0\n",
      "Training Batch 1 Classes: 1 0 0 1 0 0 1 0 0 1\n",
      "Training Batch 2 Classes: 0 1 0 1 0 1 1 1 0 0\n",
      "Training Batch 3 Classes: 0 0 0 1 1 0 0 0 0 1\n",
      "Training Batch 4 Classes: 1 0 1 0 1 0 0 0 0 0\n",
      "Image 0000-0-A.tif found in training set\n",
      "--------------------------------------------------\n",
      "Epoch: 1/2\n",
      "Images in training set: 1715 (69.97% of total dataset)\n",
      "Images in validation set: 245 (10.00% of total dataset)\n",
      "Images in test set: 491 (20.03% of total dataset)\n",
      "----------\n",
      "Total images in dataset: 2451\n",
      "Classes in training set:\n",
      "1117 without metastasis (65.13% of training set)\n",
      "598 with metastasis (34.87% of training set)\n",
      "\n",
      "Classes in validation set:\n",
      "320 without metastasis (65.17% of validation set)\n",
      "171 with metastasis (34.83% of validation set)\n",
      "\n",
      "Classes in test set:\n",
      "160 without metastasis (65.31% of test set)\n",
      "85 with metastasis (34.69% of test set)\n",
      "----------\n",
      "Total classes in dataset:\n",
      "1597 without metastasis (65.16% of dataset)\n",
      "854 with metastasis (34.84% of dataset)\n",
      "Training Batch 0 Classes: 0 0 1 0 0 1 0 1 0 0\n",
      "Training Batch 1 Classes: 1 0 0 1 0 1 0 1 1 1\n",
      "Training Batch 2 Classes: 0 0 1 0 1 0 0 0 0 1\n",
      "Training Batch 3 Classes: 1 0 1 1 0 0 1 0 1 0\n",
      "Training Batch 4 Classes: 1 1 0 0 0 1 1 0 1 0\n",
      "Image 0000-0-A.tif found in training set\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "batch_size = 10\n",
    "\n",
    "# Sample code to verify that data is shuffled into test/training/val across multiple epochs\n",
    "# Ex. In epoch 1, image A can be in training; in epoch 2, image A can be in test; etc.\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch}/{epochs-1}\")\n",
    "    \n",
    "    random_seed += 1\n",
    "    \n",
    "    train_indices, val_indices, test_indices = get_set_indices(random_seed)\n",
    "    check_set_ratios(train_indices, val_indices, test_indices)\n",
    "    train_set, val_set, test_set = get_sets(train_indices, val_indices, test_indices)\n",
    "    train_loader, val_loader, test_loader = get_loaders(train_set, val_set, test_set, batch_size)\n",
    "    \n",
    "    for batch_index, batch_data in enumerate(train_loader):\n",
    "        # Get the inputs\n",
    "        batch_images, batch_labels, batch_filenames = batch_data\n",
    "        \n",
    "        # Print labels in first 5 batches of current training set\n",
    "        if (batch_index < 5):\n",
    "            print(f\"Training Batch {batch_index} Classes: \" + ' '.join(f'{batch_labels[j]}' for j in range(len(batch_labels))))\n",
    "        \n",
    "        # Verify if a chosen image is in test, train, or val to verify \n",
    "        # if data points can be in different sets across different epochs\n",
    "        if ('0000-0-A.tif' in batch_filenames):\n",
    "            print(\"Image 0000-0-A.tif found in training set\")\n",
    "    \n",
    "    for batch_index, batch_data in enumerate(val_loader):\n",
    "        # Get the inputs\n",
    "        batch_images, batch_labels, batch_filenames = batch_data\n",
    "        \n",
    "        # Verify if a chosen image is in test, train, or val to verify \n",
    "        # if data points can be in different sets across different epochs\n",
    "        if ('0000-0-A.tif' in batch_filenames):\n",
    "            print(\"Image 0000-0-A.tif found in validation set\")\n",
    "    \n",
    "    for batch_index, batch_data in enumerate(test_loader):\n",
    "        # Get the inputs\n",
    "        batch_images, batch_labels, batch_filenames = batch_data\n",
    "        \n",
    "        # Verify if a chosen image is in test, train, or val to verify \n",
    "        # if data points can be in different sets across different epochs\n",
    "        if ('0000-0-A.tif' in batch_filenames):\n",
    "            print(\"Image 0000-0-A.tif found in test set\")\n",
    "    \n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
